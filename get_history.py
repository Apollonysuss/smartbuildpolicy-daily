import requests
import json
import xml.etree.ElementTree as ET
import time
import datetime
import os

# è¿™é‡Œä¼šè‡ªåŠ¨è¯»å–ä½ åœ¨ GitHub è®¾ç½®é‡Œå­˜å¥½çš„ Key
API_KEY = os.environ.get("DEEPSEEK_API_KEY")

def fetch_history(keyword):
    print(f"ğŸ” æ­£åœ¨æŒ–æ˜: {keyword} ...")
    # ä½¿ç”¨ Google News RSS ä¸­æ–‡ç‰ˆ
    url = f"https://news.google.com/rss/search?q={keyword}&hl=zh-CN&gl=CN&ceid=CN:zh-CN"
    try:
        resp = requests.get(url, timeout=15)
        root = ET.fromstring(resp.content)
        items = []
        for item in root.findall('./channel/item'):
            title = item.find('title').text
            link = item.find('link').text
            try:
                # å°è¯•è§£ææ—¶é—´
                dt = datetime.datetime.strptime(item.find('pubDate').text[:16], '%a, %d %b %Y')
                date_str = dt.strftime('%Y-%m-%d')
            except:
                date_str = "2023-01-01"
            
            # æ¸…ç†æ¥æº
            source = "å†å²å›é¡¾"
            if "-" in title:
                source = title.split("-")[-1].strip()
                title = title.replace(f"- {source}", "").strip()

            items.append({"title": title, "link": link, "date": date_str, "source": source})
        return items
    except Exception as e:
        print(f"âŒ æŒ–æ˜å¤±è´¥: {e}")
        return []

def call_ai(text):
    if not API_KEY: return "æ— æ‘˜è¦"
    # ä½¿ç”¨æ™ºè°±AI (å…è´¹ä¸”ç¨³)ï¼Œå¦‚æœä½ æ˜¯ DeepSeek å®˜æ–¹ï¼Œè¯·æ”¹å›å®˜æ–¹åœ°å€
    url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {API_KEY}"}
    payload = {
        "model": "glm-4-flash", # æ™ºè°±å…è´¹æ¨¡å‹
        "messages": [{"role": "user", "content": f"ä¸€å¥è¯æ¦‚æ‹¬ï¼š{text}"}],
        "stream": False
    }
    try:
        res = requests.post(url, headers=headers, json=payload, timeout=10)
        return res.json()['choices'][0]['message']['content']
    except:
        return "æ‘˜è¦ç”Ÿæˆä¸­..."

def main():
    # 1. å®šä¹‰å…³é”®è¯ (ä½ å¯ä»¥éšæ—¶å›æ¥ä¿®æ”¹è¿™é‡Œï¼Œå¢åŠ æ–°è¯)
    keywords = ["æ™ºèƒ½å»ºé€ æ”¿ç­– 2024", "å»ºç­‘æœºå™¨äºº æ¡ˆä¾‹", "æ™ºèƒ½å»ºé€  è¯•ç‚¹"]
    
    new_items = []
    for kw in keywords:
        new_items.extend(fetch_history(kw))
        time.sleep(1) 

    # 2. è¯»å–ç°æœ‰æ•°æ®
    if os.path.exists('data.json'):
        with open('data.json', 'r', encoding='utf-8') as f:
            try: old_data = json.load(f)
            except: old_data = []
    else:
        old_data = []

    # 3. åˆå¹¶
    seen = set(i['title'] for i in old_data)
    final_data = old_data
    
    count = 0
    for item in new_items:
        if item['title'] in seen:
            continue
        
        print(f"æ–°å‘ç°: {item['title'][:10]}...")
        item['summary'] = call_ai(item['title'])
        final_data.append(item)
        seen.add(item['title'])
        count += 1
        time.sleep(0.5)

    # æ’åºå¹¶ä¿å­˜
    final_data.sort(key=lambda x: x['date'], reverse=True)
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… æˆåŠŸå­˜å…¥ {count} æ¡å†å²æ•°æ®ï¼")

if __name__ == "__main__":
    main()
